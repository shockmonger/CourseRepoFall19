{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Dimensionality Reduction & Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment for this example (importing packages and configuring options)\n",
    "ensure your librosa version (printed below next cell) is 0.7 <br>\n",
    "otherwise use the following command  <br>\n",
    "*conda install -c conda-forge librosa=0.7.0*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6.2\n"
     ]
    }
   ],
   "source": [
    "#import numpy\n",
    "import numpy as np\n",
    "\n",
    "#import librosa and display the library verion installed in yoru system\n",
    "import librosa, librosa.display\n",
    "print(librosa.__version__)\n",
    "\n",
    "#import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Render plots interactively in the notebook (not a must)\n",
    "#alternatively use matplotlib inline or matplotlib notebook or matplotlib nbagg\n",
    "%matplotlib inline\n",
    "\n",
    "#select a different color-scheme for the plots\n",
    "import matplotlib.style as ms\n",
    "ms.use('seaborn-muted')\n",
    "\n",
    "#importing audio widget from IPython.display for audio playback\n",
    "from IPython.display import Audio\n",
    "\n",
    "#import scipy or scientific python\n",
    "import scipy\n",
    "\n",
    "#import os (helps retrieve the file names from the directory structure on your computer, and much more)\n",
    "import os\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "#iimport scikit-learn\n",
    "import sklearn\n",
    "\n",
    "#importing pandas for being able to load data from files such as comma separated values files\n",
    "import pandas as pd\n",
    "\n",
    "#import pathlib to easily write a function to work on all the files in a folder\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset including features from raw data.\n",
    "\n",
    "When working with ML we usually handle large amount of files. Handling them individually is not an option. Learning how to handle them via programming (using dedicated packages) is the most efficient way.\n",
    "In the ./Data/drum_sample foler there are 84 .wav files. 42 contains an acoustic or synthetic kick drum (0 to 41), the remaining 42 contains an acoustic or synthetic snare drum (0 to 41).\n",
    "\n",
    "We will use this dataset for today's examples on dimensionality reduction and classification via ML (i.e. recognizing whether a sound is a snare or a kick). Mind that this dataset has an equal number of sample in each class (and you should always aim for this, minor variations are fine, e.g. 5-10%).\n",
    "\n",
    "The raw data is organized in a way that the label (i.e. snare or drum) can be derived from the filename. This is a very handy way of handling data in supervised learning.\n",
    "\n",
    "The code below loads all samples and computes a set of scalar features (using librosa) for all files, and store these into a handy Pandas data structure.\n",
    "\n",
    "You can use a similar approach with any kind of data your are loading from files.\n",
    "\n",
    "The code below uses several functions from packages that may be new to you. Please refer to the official documentaton for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "#The next line of code includes an inline for loop \n",
    "#which will load all .mp3 samples starting with kick into kick_signals.\n",
    "#The * star is also called wildcard, the librosa.load will be performed on all files which name starts\n",
    "#with kick and ends with .wav (e.g. kick_03.wav, but also kick_adsugds.wav which we do not have in the folder).\n",
    "#Mind that the sampling rate is the default one (do you remember the value?).\n",
    "#The code below will actually create a LIST (squared brackets in Python) of Numpy arrays.\n",
    "#Te have to take this approach because we are not sure that if files have the same number of samples (they do not)\n",
    "#othrwise we could use a matrix (aka N dimensional Numpy array).\n",
    "kick_signals = [ librosa.load(p, mono=True)[0] for p in Path().glob('Data/drum_samples/kick*.wav') ]\n",
    "\n",
    "\n",
    "#Repeating the same for snare samples\n",
    "snare_signals = [ librosa.load(p, mono=True)[0] for p in Path().glob('Data/drum_samples/snare*.wav') ]\n",
    "\n",
    "#Printing the size (lenght, using len() ) of the lists which includes kick and snares (separately).\n",
    "#Does the number make sense versus what you have in the drum_samples folder?\n",
    "print(len(kick_signals))\n",
    "print(len(snare_signals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'librosa.feature' has no attribute 'rms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d75e26ae753a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#to be precise, we are still storing data into a list [], and then we use the function np.array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#to convert the list into an array (we need Numpy arrays for our ML algorithm, not lists)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mkick_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkick_signals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#Repearing the same for the snare samples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d75e26ae753a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#to be precise, we are still storing data into a list [], and then we use the function np.array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#to convert the list into an array (we need Numpy arrays for our ML algorithm, not lists)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mkick_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkick_signals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#Repearing the same for the snare samples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d75e26ae753a>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(signal)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspectral_centroid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspectral_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspectral_flatness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     ]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'librosa.feature' has no attribute 'rms'"
     ]
    }
   ],
   "source": [
    "#Instead of writing the code to extract the features we define a function,\n",
    "#which is more elegant, it's reusable (shorter code) and makes the following code more readable.\n",
    "#All features (5 of them) are from librosa and are all scalar (we take the mean over multiple blocks).\n",
    "#We have to do this (use average) because we did not check if all files have the same lenght (actually they are different).\n",
    "#Different file lenght generates Numpy arrays of different lenght (not comparable)\n",
    "#The function returns a list containing the mean of the features on \"signal\", which is the parameter we pass to the function\n",
    "#Mind that these features may not be the best to perform the classification task (it's just an example!)\n",
    "def extract_features(signal):\n",
    "\n",
    "    return [\n",
    "        np.mean(librosa.feature.zero_crossing_rate(signal)),\n",
    "        np.mean(librosa.feature.spectral_centroid(signal)),\n",
    "        np.mean(librosa.feature.spectral_contrast(signal)),\n",
    "        np.mean(librosa.feature.rms(signal)),\n",
    "        np.mean(librosa.feature.spectral_flatness(signal)),\n",
    "    ]\n",
    "\n",
    "\n",
    "#Extracting our the 5 scalar features for all kick samples.\n",
    "#Ee are using another inline for loop (this is very convenient when working with lists).\n",
    "#Now we can store the data on an Numpy array because the size of the data is consistent,\n",
    "#indeed we will have 5 numbers (features) per sample\n",
    "#to be precise, we are still storing data into a list [], and then we use the function np.array\n",
    "#to convert the list into an array (we need Numpy arrays for our ML algorithm, not lists)\n",
    "kick_features = np.array([extract_features(x) for x in kick_signals])\n",
    "\n",
    "#Repearing the same for the snare samples.\n",
    "snare_features = np.array([extract_features(x) for x in snare_signals])\n",
    "\n",
    "#Displaying the size of the Numpy arrays (this time we use the .shape attribute)\n",
    "#Check if the printed numbers are the expected ones (what's on the rows and what's on the columns?)\n",
    "print('Size of Numpy arrays for kick and snare features')\n",
    "print(kick_features.shape, snare_features.shape)\n",
    "\n",
    "#Now we create an array of labels, we can use zeros for the kicks and ones for the snare (or any other number).\n",
    "#This will help us to discriminate set of featires associated with kicks and snares\n",
    "#We can opt for \"text\" labels but this is not convenient,\n",
    "#It wont work well with neural networks, and we put \"text\" labels in Numpy arrays\n",
    "\n",
    "#Create a row of zeroes as long as the number of kick samples\n",
    "kicklabels = np.zeros(kick_features.shape[0])\n",
    "\n",
    "#Create a row of ones as long as the number of snare samples\n",
    "snarelabels = np.ones(snare_features.shape[0])\n",
    "\n",
    "#Now we concatenate (attach) the numeric labels into a single array,\n",
    "#and we also concatenate the two set of features\n",
    "labels = np.concatenate((kicklabels,snarelabels))\n",
    "features = np.concatenate((kick_features,snare_features))\n",
    "\n",
    "#check the output and reconsile these with what we just did\n",
    "print('Size of labels array',labels.shape)\n",
    "print('Size of feature array',features.shape)\n",
    "\n",
    "#Here we use the scale function of scikit-learn to scale the features,\n",
    "#this is important when using hetherogeneous (different) scalar features.\n",
    "#After this step all features will present zero mean and unit veriance (i.e. they are more comparable).\n",
    "#It is way less recommended to do this with vectorial features.\n",
    "#In this case we are overwiting the previous Numpy variable (or array) instead of creating a new one\n",
    "#(it is fine if you do not need the old data anymore, and it reduces the number of variables you use in a program)\n",
    "features = sklearn.preprocessing.scale(features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Moving the data into a Pandas structure and we assign name to each column (features are on column)\n",
    "dataset = pd.DataFrame(features)\n",
    "dataset.columns = ['ZC','SpecCen','SpecCon','RMS','SpecFlat']\n",
    "\n",
    "#Sticking an extra colum as labels\n",
    "dataset['Label'] = labels\n",
    "\n",
    "#this will display the Pandas data structure\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "a) Modify the code above (copy and paste it in the cell below) in order to add another scalar feature of your choice to the feature set (this will require modifcation in different points fo your program)\n",
    "\n",
    "b) Try to display on screen feature instead of dataset (i.e. a Numpy data structure vs a Pandas data structure). Which one is more readable?\n",
    "\n",
    "c) Pandas data structure are very handy because can be saved and loaded from files (you can load your samples only once, compute the features, and store them in a file, because it's time consuming, then you can try different ML options just by reloading the pandas data sructure from a file). Try to write and read to/from file as CSV format usint [to_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html) and [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "Principal component analysis (PCA) is a procedure that transforms a number of (possibly) correlated variables into a (smaller) number of uncorrelated variables called principal components. \n",
    "The first principal component accounts for as much of the variability in the data as possible, and each succeeding component accounts for as much of the remaining variability as possible.\n",
    "\n",
    "In the example below we keep 95% of the variance, and we use the [PCA in scikit-learn](https://scikit-learn.org/stable/modules/decomposition.html#pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an instance of a PCA object, which is an object capable of learning and applying PCA from/to data.\n",
    "#We instruct the PCA to keep only 2 components\n",
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "\n",
    "#This will learn PCA projection from data.\n",
    "#mind that we are using the Numpy data structure (without labels!!!!!)\n",
    "pca.fit(features)\n",
    "\n",
    "#Now we project the data into a from a 7D to a 2D space,\n",
    "#you can do this again and again on NEW data (e.g. test data, while PCA projection was learnt from training data)\n",
    "projected_features_pca = pca.transform(features)\n",
    "\n",
    "#Plot the distribution of the data according to the first two principle components\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for lab, col in zip(('Kick','Snare'),\n",
    "                        ('blue', 'green')):\n",
    "        plt.scatter(projected_features_pca[y==lab, 0],\n",
    "                    projected_features_pca[y==lab, 1],\n",
    "                    label=lab,\n",
    "                    c=col)\n",
    "    plt.xlabel('1st Principal Component')\n",
    "    plt.ylabel('2nd Principal Component')\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the features before projection (you should try different combinations)\n",
    "#by changing the feature names at lines 16 and 17 (select1 and select2)\n",
    "\n",
    "#we also include an histrogram (different view of the same data)\n",
    "\n",
    "#Create a dictionary of features so that we can toggle between indexes and labels for each individual feature\n",
    "feature_dict = {'ZC':0,\n",
    "                'SpecCen':1,\n",
    "                'SpecCon':2,\n",
    "                'RMS':3,\n",
    "                'SpecFlat':4}\n",
    "\n",
    "select1 = 'RMS'\n",
    "select2 = 'ZC'\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for lab, col in zip(('Kick','Snare'),\n",
    "                        ('blue', 'green')):\n",
    "        plt.scatter(features[y==lab, feature_dict[select1]],\n",
    "                    features[y==lab, feature_dict[select2]],\n",
    "                    label=lab,\n",
    "                    c=col)\n",
    "    plt.xlabel(select1)\n",
    "    plt.ylabel(select2)\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "a) Reflect on the scatter plots displayed above, and test to plot (last one) using different features (change select1 and select2). Does the feature of the differen classes (kick and snare) appears to be separable or not (both before and after PCA).\n",
    "\n",
    "b) in the example above we decided to keep only 2 principal components (easier to display on screen). Remove \"n_components=2\". Now the PCA projection wil have as many components as the original data. Add to the code \"print(pca.explained_variance_ratio_)\", this will show you the variance of each component. Can you work out (by inspection or (better) by coding how many principal coponents accounts for 90% of the total variance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis\n",
    "When using labeled data (supervised) LDA may provide better perfrmances than PCA (i.e. better separation).\n",
    "In the example below we use LDA in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an instance of the LDA obhect, which is an object capable of learning and applying LDA from/to data.\n",
    "#We instruct the LDA to keep only 2 components\n",
    "lda = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "\n",
    "#This will learn LDA projection from data.\n",
    "#LDA needs features and labels (as numbers)\n",
    "lda.fit(features, labels)\n",
    "\n",
    "\n",
    "#Now we project the data into a from a 7D to a 1D space,\n",
    "#indeed in scikit-learn, LDA transform the data to a number of dimensions\n",
    "#equal to min(n_classes - 1, n_features), which in our case is 2 classes -1 = 1.\n",
    "#with more classess we will have more dimensions at the output of LDA projection\n",
    "#you can do this again and again on NEW data (e.g. test data, while LDA projection was learnt from training data)\n",
    "projected_features_lda = lda.transform(features)\n",
    "\n",
    "#Since we have only 1 dimension we plot all points on along a line (the value on the \n",
    "#vertical axis is zero)\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for lab, col in zip(('Kick','Snare'),\n",
    "                        ('blue', 'green')):\n",
    "        plt.scatter(projected_features_lda[y==lab, 0],\n",
    "                    np.zeros(np.size(projected_features_lda[y==lab, 0])), #create an array of zero for the vert. axis\n",
    "                    label=lab,\n",
    "                    c=col)\n",
    "    plt.xlabel('1st LDA Component')\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Compare the result of LDA against the result of PCA. For a fair comparison, you can just look at the 1st Principal Component, and plot it in the same way we just plotted the LDA projection, i.e. with zero values for the vertical axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers\n",
    "In the examples below we use [Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html), [KNN](https://scikit-learn.org/stable/modules/neighbors.html), and [SVN](https://scikit-learn.org/stable/modules/svm.html) from scikit-learn to classify the features exctracted from the kick and snare samples (the task is already quite simple, so we wont apply PCA or LDA, but you will do this later as an excercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before proceeding with training and testing of the classifiers\n",
    "#we split the data in training and testing set using a 70/30 partitioning.\n",
    "#This is done a useful function in scikit-learn (called train_test_split)\n",
    "#The partitioning is done randomly but starting from a seed you can specify (random_state)\n",
    "#Every time you change the starting random state value, you will experience\n",
    "#a different partitioning and (likely) a different classification result.\n",
    "\n",
    "#Importing the tool,\n",
    "#alternatively you can call sklearn.model_selection.train_test_split(..,..,..,)\n",
    "#but that will be too long\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#splitting the dataset in training and testing parts\n",
    "feat_train, feat_test, lab_train, lab_test = train_test_split(features, labels, test_size=0.3, random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an instance of a Gaussian Naive Bayes classifier\n",
    "model_GNB = sklearn.naive_bayes.GaussianNB()\n",
    "\n",
    "#training the model\n",
    "model_GNB.fit(feat_train, lab_train)\n",
    "\n",
    "#applying the the model on the test data (features)\n",
    "lab_predict =  model_GNB.predict(feat_test)\n",
    "\n",
    "\n",
    "#print the number of misclassified samples, accuracy and complete report (using scikit learn metric tools) \n",
    "print('Number of mislabeled samples %d out of %d' % ((lab_test != lab_predict).sum(),lab_test.size))\n",
    "print('Accuracy:',sklearn.metrics.accuracy_score(lab_test, lab_predict))\n",
    "print(sklearn.metrics.classification_report(lab_test, lab_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an instance of a KNN classifier\n",
    "#and setting it to classify according to only 1 nearest neighbor\n",
    "model_KNN = sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "#training the model\n",
    "model_KNN.fit(feat_train, lab_train)\n",
    "\n",
    "#applying the the model on the test data (features)\n",
    "lab_predict =  model_KNN.predict(feat_test)\n",
    "\n",
    "\n",
    "#print the number of misclassified samples, accuracy and complete report (using scikit learn metric tools) \n",
    "print('Number of mislabeled samples %d out of %d' % ((lab_test != lab_predict).sum(),lab_test.size))\n",
    "print('Accuracy:',sklearn.metrics.accuracy_score(lab_test, lab_predict))\n",
    "print(sklearn.metrics.classification_report(lab_test, lab_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an instance of a SVN classifier\n",
    "#and setting it to use a linear kernel\n",
    "model_SVN = sklearn.svm.SVC(kernel='linear')\n",
    "\n",
    "#training the model\n",
    "model_SVN.fit(feat_train, lab_train)\n",
    "\n",
    "#applying the the model on the test data (features)\n",
    "lab_predict =  model_SVN.predict(feat_test)\n",
    "\n",
    "\n",
    "#print the number of misclassified samples, accuracy and complete report (using scikit learn metric tools) \n",
    "print('Number of mislabeled samples %d out of %d' % ((lab_test != lab_predict).sum(),lab_test.size))\n",
    "print('Accuracy:',sklearn.metrics.accuracy_score(lab_test, lab_predict))\n",
    "print(sklearn.metrics.classification_report(lab_test, lab_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "a) The results of the classifier is pretty good, as the task that we set in this example was fairly simple.\n",
    "To get worst results (just for the sake of getting familiar with the classification report) you can try to significantly increase the test set (go from 70/30 to 30/70 or 20/80) and/or add some noise to the features.\n",
    "To add noise replace the following line of code\n",
    "\n",
    "... = train_test_split(features, labels, test_size=0.5, random_state=14)\n",
    "\n",
    "with\n",
    "\n",
    "... = train_test_split(features + 10*np.random.rand(features.shape[0], features.shape[1]), labels, test_size=0.5, random_state=14)\n",
    "\n",
    "where the 10 represent the amount of noise (use numbers in the range 1 to 20)\n",
    "\n",
    "\n",
    "b) Try to change settings of the classifiers, such as the number of neighbors used in KNN and the kernel of SVM (refer to the official documentation).\n",
    "\n",
    "c) Try to use PCA or LDA dimensionality reduction before using the classifier. Mind that in the earlier examples we used the entire dataset to learn the dimensionality reduction (i.t. the function fit()), which is not a fair approach. You should learn the dimensionality reduction only by using the training set, and then use it to project (i.e. transform()) both training anr test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
