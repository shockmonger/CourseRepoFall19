{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: Neural Networks (Multi Layer Perceptron)\n",
    "\n",
    "In this example we use Multi Layer Perceptron to classify audio samples in three different classes: kick drum, snare drum, and cymbal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment for this example (importing packages and configuring options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6.2\n"
     ]
    }
   ],
   "source": [
    "#import numpy\n",
    "import numpy as np\n",
    "\n",
    "#import librosa and display the library verion installed in yoru system\n",
    "import librosa, librosa.display\n",
    "print(librosa.__version__)\n",
    "\n",
    "#import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Render plots interactively in the notebook (not a must)\n",
    "#alternatively use matplotlib inline or matplotlib notebook or matplotlib nbagg\n",
    "%matplotlib inline\n",
    "\n",
    "#select a different color-scheme for the plots\n",
    "import matplotlib.style as ms\n",
    "ms.use('seaborn-muted')\n",
    "\n",
    "#importing audio widget from IPython.display for audio playback\n",
    "from IPython.display import Audio\n",
    "\n",
    "#import scipy or scientific python\n",
    "import scipy\n",
    "\n",
    "#import os (helps retrieve the file names from the directory structure on your computer, and much more)\n",
    "import os\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "#importing pandas for being able to load data from files such as comma separated values files\n",
    "import pandas as pd\n",
    "\n",
    "#import pathlib to easily write a function to work on all the files in a folder\n",
    "from pathlib import Path\n",
    "\n",
    "#importing scikit learn library for learning\n",
    "import sklearn\n",
    "\n",
    "#enable auto completion in jupiter notebook\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset including features from raw data.\n",
    "This is a repetition of the code in the Day 2 Jupiter Notebook. However, this time the dataset inclused also 42 cymbal samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "42\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "#The next line of code includes an inline for loop \n",
    "#which will load all .wav samples starting with kick into kick_signals.\n",
    "#The * star is also called wildcard, the librosa.load will be performed on all files which name starts\n",
    "#with kick and ends with .wav (e.g. kick_03.wav, but also kick_adsugds.wav which we do not have in the folder).\n",
    "#Mind that the sampling rate is the default one (do you remember the value?).\n",
    "#The code below will actually create a LIST (squared brackets in Python) of Numpy arrays.\n",
    "#Te have to take this approach because we are not sure that if files have the same number of samples (they do not)\n",
    "#othrwise we could use a matrix (aka N dimensional Numpy array).\n",
    "kick_signals = [ librosa.load(p, mono=True)[0] for p in Path().glob('Data/drum_samples/kick*.wav') ]\n",
    "\n",
    "#Repeating the same for snare samples\n",
    "snare_signals = [ librosa.load(p, mono=True)[0] for p in Path().glob('Data/drum_samples/snare*.wav') ]\n",
    "\n",
    "#Repeating the same for cymbal\n",
    "cymbal_signals = [ librosa.load(p, mono=True)[0] for p in Path().glob('Data/drum_samples/cymbal*.wav')]\n",
    "\n",
    "#Printing the size (lenght, using len() ) of the lists which includes kick and snares (separately).\n",
    "#Does the number make sense versus what you have in the drum_samples folder?\n",
    "print(len(kick_signals))\n",
    "print(len(snare_signals))\n",
    "print(len(cymbal_signals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Numpy arrays for kick and snare features:\n",
      "(42, 5) (42, 5) (18, 5)\n",
      "Size of labels array (102,)\n",
      "Size of feature array (102, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZC</th>\n",
       "      <th>SpecCen</th>\n",
       "      <th>SpecCon</th>\n",
       "      <th>RMS</th>\n",
       "      <th>SpecFlat</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.938593</td>\n",
       "      <td>-1.238997</td>\n",
       "      <td>-0.378333</td>\n",
       "      <td>-0.494040</td>\n",
       "      <td>1.160168</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.856175</td>\n",
       "      <td>-0.813473</td>\n",
       "      <td>-0.729564</td>\n",
       "      <td>-0.509628</td>\n",
       "      <td>1.333756</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.968865</td>\n",
       "      <td>-1.053241</td>\n",
       "      <td>-0.014963</td>\n",
       "      <td>-0.624725</td>\n",
       "      <td>0.872121</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.949664</td>\n",
       "      <td>-1.200352</td>\n",
       "      <td>-0.701311</td>\n",
       "      <td>0.183344</td>\n",
       "      <td>1.014631</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.010076</td>\n",
       "      <td>-1.358555</td>\n",
       "      <td>0.606447</td>\n",
       "      <td>2.959739</td>\n",
       "      <td>0.555241</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.896379</td>\n",
       "      <td>-1.130501</td>\n",
       "      <td>-0.532440</td>\n",
       "      <td>-0.230217</td>\n",
       "      <td>1.728487</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.944879</td>\n",
       "      <td>-0.990040</td>\n",
       "      <td>-0.263528</td>\n",
       "      <td>-0.534566</td>\n",
       "      <td>0.733316</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.954551</td>\n",
       "      <td>-0.984236</td>\n",
       "      <td>-0.768048</td>\n",
       "      <td>-0.603973</td>\n",
       "      <td>0.699822</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.041591</td>\n",
       "      <td>-1.357617</td>\n",
       "      <td>-0.312265</td>\n",
       "      <td>-0.349562</td>\n",
       "      <td>1.111287</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.957865</td>\n",
       "      <td>-0.883718</td>\n",
       "      <td>-0.720475</td>\n",
       "      <td>-0.623423</td>\n",
       "      <td>0.700744</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.987921</td>\n",
       "      <td>-1.146164</td>\n",
       "      <td>-0.185588</td>\n",
       "      <td>-0.403841</td>\n",
       "      <td>0.887235</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.976912</td>\n",
       "      <td>-0.923121</td>\n",
       "      <td>-0.990214</td>\n",
       "      <td>-0.608872</td>\n",
       "      <td>1.098374</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.961936</td>\n",
       "      <td>-1.036971</td>\n",
       "      <td>-1.210955</td>\n",
       "      <td>-0.249712</td>\n",
       "      <td>1.485375</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.919847</td>\n",
       "      <td>-0.984448</td>\n",
       "      <td>-0.377478</td>\n",
       "      <td>-0.358572</td>\n",
       "      <td>1.677297</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.008865</td>\n",
       "      <td>-1.129275</td>\n",
       "      <td>-0.288536</td>\n",
       "      <td>-0.485144</td>\n",
       "      <td>0.677316</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.966195</td>\n",
       "      <td>-1.075800</td>\n",
       "      <td>-0.586082</td>\n",
       "      <td>-0.304644</td>\n",
       "      <td>0.741906</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.930572</td>\n",
       "      <td>-1.207700</td>\n",
       "      <td>-2.834629</td>\n",
       "      <td>-0.598112</td>\n",
       "      <td>1.513617</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.843411</td>\n",
       "      <td>-1.195552</td>\n",
       "      <td>-1.612330</td>\n",
       "      <td>-0.577770</td>\n",
       "      <td>1.132209</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.024529</td>\n",
       "      <td>-1.299504</td>\n",
       "      <td>-2.091752</td>\n",
       "      <td>-0.410779</td>\n",
       "      <td>1.351549</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.820426</td>\n",
       "      <td>-1.036326</td>\n",
       "      <td>-0.629936</td>\n",
       "      <td>0.129125</td>\n",
       "      <td>2.055580</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.987558</td>\n",
       "      <td>-1.065897</td>\n",
       "      <td>-0.963941</td>\n",
       "      <td>-0.584177</td>\n",
       "      <td>0.770680</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.904618</td>\n",
       "      <td>-1.211093</td>\n",
       "      <td>0.503802</td>\n",
       "      <td>-0.554968</td>\n",
       "      <td>1.846571</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.931890</td>\n",
       "      <td>-1.041607</td>\n",
       "      <td>-1.123719</td>\n",
       "      <td>-0.433951</td>\n",
       "      <td>1.902529</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.042622</td>\n",
       "      <td>-1.202516</td>\n",
       "      <td>-0.032667</td>\n",
       "      <td>-0.602314</td>\n",
       "      <td>1.036801</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.956295</td>\n",
       "      <td>-1.182311</td>\n",
       "      <td>0.876143</td>\n",
       "      <td>0.237380</td>\n",
       "      <td>1.476313</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.022050</td>\n",
       "      <td>-1.125361</td>\n",
       "      <td>-0.494611</td>\n",
       "      <td>-0.579642</td>\n",
       "      <td>0.606424</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.942743</td>\n",
       "      <td>-0.780903</td>\n",
       "      <td>-1.689555</td>\n",
       "      <td>-0.293109</td>\n",
       "      <td>1.357626</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.007224</td>\n",
       "      <td>-1.231232</td>\n",
       "      <td>-0.247101</td>\n",
       "      <td>-0.603115</td>\n",
       "      <td>0.662167</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.961454</td>\n",
       "      <td>-1.051953</td>\n",
       "      <td>-1.034738</td>\n",
       "      <td>-0.515221</td>\n",
       "      <td>0.889381</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.969778</td>\n",
       "      <td>-1.089140</td>\n",
       "      <td>-1.071906</td>\n",
       "      <td>-0.399676</td>\n",
       "      <td>1.022611</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-0.184391</td>\n",
       "      <td>0.141291</td>\n",
       "      <td>0.064675</td>\n",
       "      <td>0.277311</td>\n",
       "      <td>-0.937695</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.006393</td>\n",
       "      <td>1.177876</td>\n",
       "      <td>0.548650</td>\n",
       "      <td>-0.560765</td>\n",
       "      <td>-0.569961</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.602799</td>\n",
       "      <td>1.172669</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>-0.571960</td>\n",
       "      <td>-0.639209</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-0.091463</td>\n",
       "      <td>0.210792</td>\n",
       "      <td>-0.200365</td>\n",
       "      <td>1.178882</td>\n",
       "      <td>-0.901534</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.231978</td>\n",
       "      <td>0.568011</td>\n",
       "      <td>0.142455</td>\n",
       "      <td>0.940363</td>\n",
       "      <td>-0.838705</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.069966</td>\n",
       "      <td>0.843895</td>\n",
       "      <td>0.507799</td>\n",
       "      <td>-0.634642</td>\n",
       "      <td>-0.870459</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-0.198816</td>\n",
       "      <td>0.302706</td>\n",
       "      <td>0.369241</td>\n",
       "      <td>1.116975</td>\n",
       "      <td>-0.984666</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-0.462107</td>\n",
       "      <td>0.329286</td>\n",
       "      <td>0.117484</td>\n",
       "      <td>0.428837</td>\n",
       "      <td>-0.967183</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.314195</td>\n",
       "      <td>0.484894</td>\n",
       "      <td>-0.386474</td>\n",
       "      <td>1.665944</td>\n",
       "      <td>-0.853813</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.100746</td>\n",
       "      <td>0.708504</td>\n",
       "      <td>-0.426835</td>\n",
       "      <td>0.785573</td>\n",
       "      <td>-0.652157</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.286576</td>\n",
       "      <td>0.784607</td>\n",
       "      <td>0.299837</td>\n",
       "      <td>0.365807</td>\n",
       "      <td>-0.848398</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.720012</td>\n",
       "      <td>0.700445</td>\n",
       "      <td>-0.026159</td>\n",
       "      <td>1.737060</td>\n",
       "      <td>-0.849741</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-0.212120</td>\n",
       "      <td>-0.411883</td>\n",
       "      <td>1.130688</td>\n",
       "      <td>-0.980515</td>\n",
       "      <td>-1.036898</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.999594</td>\n",
       "      <td>0.671259</td>\n",
       "      <td>1.011686</td>\n",
       "      <td>-0.926651</td>\n",
       "      <td>-0.886976</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.554524</td>\n",
       "      <td>1.008718</td>\n",
       "      <td>0.923609</td>\n",
       "      <td>-1.015909</td>\n",
       "      <td>-0.628664</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.386039</td>\n",
       "      <td>-0.038268</td>\n",
       "      <td>1.280391</td>\n",
       "      <td>-0.931054</td>\n",
       "      <td>-1.005848</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.124156</td>\n",
       "      <td>0.797712</td>\n",
       "      <td>1.195505</td>\n",
       "      <td>-0.649880</td>\n",
       "      <td>-0.783887</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.184621</td>\n",
       "      <td>0.772196</td>\n",
       "      <td>3.732727</td>\n",
       "      <td>-0.956300</td>\n",
       "      <td>-1.029375</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-0.607698</td>\n",
       "      <td>-0.903027</td>\n",
       "      <td>1.303337</td>\n",
       "      <td>-0.958037</td>\n",
       "      <td>-1.050052</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.534146</td>\n",
       "      <td>0.845324</td>\n",
       "      <td>0.626246</td>\n",
       "      <td>-0.800097</td>\n",
       "      <td>-0.760229</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2.428674</td>\n",
       "      <td>1.473848</td>\n",
       "      <td>0.326535</td>\n",
       "      <td>-0.681665</td>\n",
       "      <td>-0.529086</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>3.387253</td>\n",
       "      <td>2.275581</td>\n",
       "      <td>2.306408</td>\n",
       "      <td>-0.737867</td>\n",
       "      <td>-0.867761</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.914023</td>\n",
       "      <td>1.098452</td>\n",
       "      <td>0.713193</td>\n",
       "      <td>-0.634965</td>\n",
       "      <td>-0.686050</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.264986</td>\n",
       "      <td>0.630616</td>\n",
       "      <td>3.332926</td>\n",
       "      <td>-0.889551</td>\n",
       "      <td>-1.023529</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2.058604</td>\n",
       "      <td>1.244148</td>\n",
       "      <td>0.270387</td>\n",
       "      <td>-0.635164</td>\n",
       "      <td>-0.570694</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.107048</td>\n",
       "      <td>0.483578</td>\n",
       "      <td>-0.606338</td>\n",
       "      <td>-0.976600</td>\n",
       "      <td>-0.618456</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2.213361</td>\n",
       "      <td>1.521115</td>\n",
       "      <td>2.984724</td>\n",
       "      <td>-0.956693</td>\n",
       "      <td>-0.974396</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.793479</td>\n",
       "      <td>1.090304</td>\n",
       "      <td>0.726147</td>\n",
       "      <td>-0.958186</td>\n",
       "      <td>-0.628492</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.674175</td>\n",
       "      <td>0.709227</td>\n",
       "      <td>1.415384</td>\n",
       "      <td>-0.842281</td>\n",
       "      <td>-0.882642</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2.184222</td>\n",
       "      <td>1.701741</td>\n",
       "      <td>1.265991</td>\n",
       "      <td>-1.014449</td>\n",
       "      <td>-0.602707</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ZC   SpecCen   SpecCon       RMS  SpecFlat  Label\n",
       "0   -0.938593 -1.238997 -0.378333 -0.494040  1.160168    0.0\n",
       "1   -0.856175 -0.813473 -0.729564 -0.509628  1.333756    0.0\n",
       "2   -0.968865 -1.053241 -0.014963 -0.624725  0.872121    0.0\n",
       "3   -0.949664 -1.200352 -0.701311  0.183344  1.014631    0.0\n",
       "4   -1.010076 -1.358555  0.606447  2.959739  0.555241    0.0\n",
       "5   -0.896379 -1.130501 -0.532440 -0.230217  1.728487    0.0\n",
       "6   -0.944879 -0.990040 -0.263528 -0.534566  0.733316    0.0\n",
       "7   -0.954551 -0.984236 -0.768048 -0.603973  0.699822    0.0\n",
       "8   -1.041591 -1.357617 -0.312265 -0.349562  1.111287    0.0\n",
       "9   -0.957865 -0.883718 -0.720475 -0.623423  0.700744    0.0\n",
       "10  -0.987921 -1.146164 -0.185588 -0.403841  0.887235    0.0\n",
       "11  -0.976912 -0.923121 -0.990214 -0.608872  1.098374    0.0\n",
       "12  -0.961936 -1.036971 -1.210955 -0.249712  1.485375    0.0\n",
       "13  -0.919847 -0.984448 -0.377478 -0.358572  1.677297    0.0\n",
       "14  -1.008865 -1.129275 -0.288536 -0.485144  0.677316    0.0\n",
       "15  -0.966195 -1.075800 -0.586082 -0.304644  0.741906    0.0\n",
       "16  -0.930572 -1.207700 -2.834629 -0.598112  1.513617    0.0\n",
       "17  -0.843411 -1.195552 -1.612330 -0.577770  1.132209    0.0\n",
       "18  -1.024529 -1.299504 -2.091752 -0.410779  1.351549    0.0\n",
       "19  -0.820426 -1.036326 -0.629936  0.129125  2.055580    0.0\n",
       "20  -0.987558 -1.065897 -0.963941 -0.584177  0.770680    0.0\n",
       "21  -0.904618 -1.211093  0.503802 -0.554968  1.846571    0.0\n",
       "22  -0.931890 -1.041607 -1.123719 -0.433951  1.902529    0.0\n",
       "23  -1.042622 -1.202516 -0.032667 -0.602314  1.036801    0.0\n",
       "24  -0.956295 -1.182311  0.876143  0.237380  1.476313    0.0\n",
       "25  -1.022050 -1.125361 -0.494611 -0.579642  0.606424    0.0\n",
       "26  -0.942743 -0.780903 -1.689555 -0.293109  1.357626    0.0\n",
       "27  -1.007224 -1.231232 -0.247101 -0.603115  0.662167    0.0\n",
       "28  -0.961454 -1.051953 -1.034738 -0.515221  0.889381    0.0\n",
       "29  -0.969778 -1.089140 -1.071906 -0.399676  1.022611    0.0\n",
       "..        ...       ...       ...       ...       ...    ...\n",
       "72  -0.184391  0.141291  0.064675  0.277311 -0.937695    1.0\n",
       "73   1.006393  1.177876  0.548650 -0.560765 -0.569961    1.0\n",
       "74   0.602799  1.172669  0.001174 -0.571960 -0.639209    1.0\n",
       "75  -0.091463  0.210792 -0.200365  1.178882 -0.901534    1.0\n",
       "76   0.231978  0.568011  0.142455  0.940363 -0.838705    1.0\n",
       "77   0.069966  0.843895  0.507799 -0.634642 -0.870459    1.0\n",
       "78  -0.198816  0.302706  0.369241  1.116975 -0.984666    1.0\n",
       "79  -0.462107  0.329286  0.117484  0.428837 -0.967183    1.0\n",
       "80   0.314195  0.484894 -0.386474  1.665944 -0.853813    1.0\n",
       "81   1.100746  0.708504 -0.426835  0.785573 -0.652157    1.0\n",
       "82   0.286576  0.784607  0.299837  0.365807 -0.848398    1.0\n",
       "83   0.720012  0.700445 -0.026159  1.737060 -0.849741    1.0\n",
       "84  -0.212120 -0.411883  1.130688 -0.980515 -1.036898    2.0\n",
       "85   0.999594  0.671259  1.011686 -0.926651 -0.886976    2.0\n",
       "86   1.554524  1.008718  0.923609 -1.015909 -0.628664    2.0\n",
       "87   0.386039 -0.038268  1.280391 -0.931054 -1.005848    2.0\n",
       "88   1.124156  0.797712  1.195505 -0.649880 -0.783887    2.0\n",
       "89   1.184621  0.772196  3.732727 -0.956300 -1.029375    2.0\n",
       "90  -0.607698 -0.903027  1.303337 -0.958037 -1.050052    2.0\n",
       "91   1.534146  0.845324  0.626246 -0.800097 -0.760229    2.0\n",
       "92   2.428674  1.473848  0.326535 -0.681665 -0.529086    2.0\n",
       "93   3.387253  2.275581  2.306408 -0.737867 -0.867761    2.0\n",
       "94   1.914023  1.098452  0.713193 -0.634965 -0.686050    2.0\n",
       "95   1.264986  0.630616  3.332926 -0.889551 -1.023529    2.0\n",
       "96   2.058604  1.244148  0.270387 -0.635164 -0.570694    2.0\n",
       "97   0.107048  0.483578 -0.606338 -0.976600 -0.618456    2.0\n",
       "98   2.213361  1.521115  2.984724 -0.956693 -0.974396    2.0\n",
       "99   1.793479  1.090304  0.726147 -0.958186 -0.628492    2.0\n",
       "100  0.674175  0.709227  1.415384 -0.842281 -0.882642    2.0\n",
       "101  2.184222  1.701741  1.265991 -1.014449 -0.602707    2.0\n",
       "\n",
       "[102 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instead of writing the code to extract the features we define a function,\n",
    "#which is more elegant, it's reusable (shorter code) and makes the following code more readable.\n",
    "#All features (5 of them) are from librosa and are all scalar (we take the mean over multiple blocks).\n",
    "#We have to do this (use average) because we did not check if all files have the same lenght (actually they are different).\n",
    "#Different file lenght generates Numpy arrays of different lenght (not comparable)\n",
    "#The function returns a list containing the mean of the features on \"signal\", which is the parameter we pass to the function\n",
    "#Mind that these features may not be the best to perform the classification task (it's just an example!)\n",
    "def extract_features(signal):\n",
    "\n",
    "    return [\n",
    "        np.mean(librosa.feature.zero_crossing_rate(signal)),\n",
    "        np.mean(librosa.feature.spectral_centroid(signal)),\n",
    "        np.mean(librosa.feature.spectral_contrast(signal)),\n",
    "        np.mean(librosa.feature.rmse(signal)),\n",
    "        np.mean(librosa.feature.spectral_flatness(signal)),\n",
    "    ]\n",
    "\n",
    "\n",
    "#Extracting our the 5 scalar features for all kick samples.\n",
    "#Ee are using another inline for loop (this is very convenient when working with lists).\n",
    "#Now we can store the data on an Numpy array because the size of the data is consistent,\n",
    "#indeed we will have 5 numbers (features) per sample\n",
    "#to be precise, we are still storing data into a list [], and then we use the function np.array\n",
    "#to convert the list into an array (we need Numpy arrays for our ML algorithm, not lists)\n",
    "kick_features = np.array([extract_features(x) for x in kick_signals])\n",
    "\n",
    "#Repearing the same for the snare samples.\n",
    "snare_features = np.array([extract_features(x) for x in snare_signals])\n",
    "\n",
    "#repeating the same for cymbal samples\n",
    "cymbal_features = np.array([extract_features(x) for x in cymbal_signals])\n",
    "\n",
    "#Displaying the size of the Numpy arrays (this time we use the .shape attribute)\n",
    "#Check if the printed numbers are the expected ones (what's on the rows and what's on the columns?)\n",
    "print('Size of Numpy arrays for kick and snare features:')\n",
    "print(kick_features.shape, snare_features.shape, cymbal_features.shape)\n",
    "\n",
    "#Now we create an array of labels, we can use zeros for the kicks and ones for the snare (or any other number).\n",
    "#This will help us to discriminate set of featires associated with kicks and snares\n",
    "#We can opt for \"text\" labels but this is not convenient,\n",
    "#It wont work well with neural networks, and we put \"text\" labels in Numpy arrays\n",
    "\n",
    "#Create a row of zeroes as long as the number of kick samples\n",
    "kicklabels = np.zeros(kick_features.shape[0])\n",
    "\n",
    "#Create a row of ones as long as the number of snare samples\n",
    "snarelabels = np.ones(snare_features.shape[0])\n",
    "\n",
    "#Create a row of twos as long as the number of cymbal samples\n",
    "cymballabels = np.full(cymbal_features.shape[0],2)\n",
    "\n",
    "#Now we concatenate (attach) the numeric labels into a single array,\n",
    "#and we also concatenate the two set of features\n",
    "labels = np.concatenate((kicklabels,snarelabels,cymballabels))\n",
    "features = np.concatenate((kick_features,snare_features,cymbal_features))\n",
    "\n",
    "#check the output and reconsile these with what we just did\n",
    "print('Size of labels array',labels.shape)\n",
    "print('Size of feature array',features.shape)\n",
    "\n",
    "#Here we use the scale function of scikit-learn to scale the features,\n",
    "#this is important when using hetherogeneous (different) scalar features.\n",
    "#After this step all features will present zero mean and unit veriance (i.e. they are more comparable).\n",
    "#It is way less recommended to do this with vectorial features.\n",
    "#In this case we are overwiting the previous Numpy variable (or array) instead of creating a new one\n",
    "#(it is fine if you do not need the old data anymore, and it reduces the number of variables you use in a program)\n",
    "features = sklearn.preprocessing.scale(features)\n",
    "\n",
    "\n",
    "#Moving the data into a Pandas structure and we assign name to each column (features are on column)\n",
    "dataset = pd.DataFrame(features)\n",
    "dataset.columns = ['ZC','SpecCen','SpecCon','RMS','SpecFlat']\n",
    "\n",
    "#Sticking an extra colum as labels\n",
    "dataset['Label'] = labels\n",
    "\n",
    "#creating a list of text labels, the list index (0,1,and 2) match with the numerical\n",
    "#labels created above for the three classes\n",
    "classes = ['kick','snare','cymbal']\n",
    "\n",
    "#this will display the Pandas data structure\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -0.938593\n",
       "1     -0.856175\n",
       "2     -0.968865\n",
       "3     -0.949664\n",
       "4     -1.010076\n",
       "5     -0.896379\n",
       "6     -0.944879\n",
       "7     -0.954551\n",
       "8     -1.041591\n",
       "9     -0.957865\n",
       "10    -0.987921\n",
       "11    -0.976912\n",
       "12    -0.961936\n",
       "13    -0.919847\n",
       "14    -1.008865\n",
       "15    -0.966195\n",
       "16    -0.930572\n",
       "17    -0.843411\n",
       "18    -1.024529\n",
       "19    -0.820426\n",
       "20    -0.987558\n",
       "21    -0.904618\n",
       "22    -0.931890\n",
       "23    -1.042622\n",
       "24    -0.956295\n",
       "25    -1.022050\n",
       "26    -0.942743\n",
       "27    -1.007224\n",
       "28    -0.961454\n",
       "29    -0.969778\n",
       "         ...   \n",
       "72    -0.184391\n",
       "73     1.006393\n",
       "74     0.602799\n",
       "75    -0.091463\n",
       "76     0.231978\n",
       "77     0.069966\n",
       "78    -0.198816\n",
       "79    -0.462107\n",
       "80     0.314195\n",
       "81     1.100746\n",
       "82     0.286576\n",
       "83     0.720012\n",
       "84    -0.212120\n",
       "85     0.999594\n",
       "86     1.554524\n",
       "87     0.386039\n",
       "88     1.124156\n",
       "89     1.184621\n",
       "90    -0.607698\n",
       "91     1.534146\n",
       "92     2.428674\n",
       "93     3.387253\n",
       "94     1.914023\n",
       "95     1.264986\n",
       "96     2.058604\n",
       "97     0.107048\n",
       "98     2.213361\n",
       "99     1.793479\n",
       "100    0.674175\n",
       "101    2.184222\n",
       "Name: ZC, Length: 102, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.ZC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_excel('dataset.xlsx',sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for classification\n",
    "The [MLPClassifier in scikit learn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation.\n",
    "\n",
    "MLP trains on two arrays: array X of size (n_samples, n_features), which holds the training samples represented as floating point feature vectors; and array y of size (n_samples,), which holds the target values (class labels) for the training samples.\n",
    "\n",
    "It is not necessary to explicitly specify one output neuron per class, this is already implemented in the MLPClassifier object. Moreover, the output is always one of the provided labels (whetever they are).\n",
    "\n",
    "The MLPClassifier can learn non-linear models and can learn models in real-time (on-line learning) using partial_fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before proceeding with training and testing of the classifiers\n",
    "#we split the data in training and testing set using a 70/30 partitioning.\n",
    "#This is done a useful function in scikit-learn (called train_test_split)\n",
    "#The partitioning is done randomly but starting from a seed you can specify (random_state)\n",
    "#Every time you change the starting random state value, you will experience\n",
    "#a different partitioning and (likely) a different classification result.\n",
    "\n",
    "#Importing the tool,\n",
    "#alternatively you can call sklearn.model_selection.train_test_split(..,..,..,)\n",
    "#but that will be too long\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#splitting the dataset in training and testing parts\n",
    "feat_train, feat_test, lab_train, lab_test = train_test_split(features, labels, test_size=0.3, random_state=214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.83831830\n",
      "Iteration 2, loss = 1.83533342\n",
      "Iteration 3, loss = 1.83235700\n",
      "Iteration 4, loss = 1.82938918\n",
      "Iteration 5, loss = 1.82643011\n",
      "Iteration 6, loss = 1.82347992\n",
      "Iteration 7, loss = 1.82053874\n",
      "Iteration 8, loss = 1.81760670\n",
      "Iteration 9, loss = 1.81468393\n",
      "Iteration 10, loss = 1.81177055\n",
      "Iteration 11, loss = 1.80886667\n",
      "Iteration 12, loss = 1.80597240\n",
      "Iteration 13, loss = 1.80308786\n",
      "Iteration 14, loss = 1.80021314\n",
      "Iteration 15, loss = 1.79734602\n",
      "Iteration 16, loss = 1.79443656\n",
      "Iteration 17, loss = 1.79153487\n",
      "Iteration 18, loss = 1.78864154\n",
      "Iteration 19, loss = 1.78575705\n",
      "Iteration 20, loss = 1.78288180\n",
      "Iteration 21, loss = 1.78001972\n",
      "Iteration 22, loss = 1.77715767\n",
      "Iteration 23, loss = 1.77434325\n",
      "Iteration 24, loss = 1.77154747\n",
      "Iteration 25, loss = 1.76876240\n",
      "Iteration 26, loss = 1.76598817\n",
      "Iteration 27, loss = 1.76322488\n",
      "Iteration 28, loss = 1.76050125\n",
      "Iteration 29, loss = 1.75778212\n",
      "Iteration 30, loss = 1.75503080\n",
      "Iteration 31, loss = 1.75228614\n",
      "Iteration 32, loss = 1.74955081\n",
      "Iteration 33, loss = 1.74682514\n",
      "Iteration 34, loss = 1.74410941\n",
      "Iteration 35, loss = 1.74140386\n",
      "Iteration 36, loss = 1.73870868\n",
      "Iteration 37, loss = 1.73602405\n",
      "Iteration 38, loss = 1.73332812\n",
      "Iteration 39, loss = 1.73056414\n",
      "Iteration 40, loss = 1.72781031\n",
      "Iteration 41, loss = 1.72510720\n",
      "Iteration 42, loss = 1.72241710\n",
      "Iteration 43, loss = 1.71976336\n",
      "Iteration 44, loss = 1.71711122\n",
      "Iteration 45, loss = 1.71443399\n",
      "Iteration 46, loss = 1.71175162\n",
      "Iteration 47, loss = 1.70909523\n",
      "Iteration 48, loss = 1.70647831\n",
      "Iteration 49, loss = 1.70384480\n",
      "Iteration 50, loss = 1.70114027\n",
      "Iteration 51, loss = 1.69846175\n",
      "Iteration 52, loss = 1.69582164\n",
      "Iteration 53, loss = 1.69322738\n",
      "Iteration 54, loss = 1.69065663\n",
      "Iteration 55, loss = 1.68809909\n",
      "Iteration 56, loss = 1.68559840\n",
      "Iteration 57, loss = 1.68311621\n",
      "Iteration 58, loss = 1.68064934\n",
      "Iteration 59, loss = 1.67819734\n",
      "Iteration 60, loss = 1.67575978\n",
      "Iteration 61, loss = 1.67333625\n",
      "Iteration 62, loss = 1.67092639\n",
      "Iteration 63, loss = 1.66849408\n",
      "Iteration 64, loss = 1.66606003\n",
      "Iteration 65, loss = 1.66363956\n",
      "Iteration 66, loss = 1.66122930\n",
      "Iteration 67, loss = 1.65882934\n",
      "Iteration 68, loss = 1.65643976\n",
      "Iteration 69, loss = 1.65406059\n",
      "Iteration 70, loss = 1.65169187\n",
      "Iteration 71, loss = 1.64933362\n",
      "Iteration 72, loss = 1.64698582\n",
      "Iteration 73, loss = 1.64467139\n",
      "Iteration 74, loss = 1.64239397\n",
      "Iteration 75, loss = 1.64012982\n",
      "Iteration 76, loss = 1.63787851\n",
      "Iteration 77, loss = 1.63563967\n",
      "Iteration 78, loss = 1.63341291\n",
      "Iteration 79, loss = 1.63119792\n",
      "Iteration 80, loss = 1.62899437\n",
      "Iteration 81, loss = 1.62680198\n",
      "Iteration 82, loss = 1.62462049\n",
      "Iteration 83, loss = 1.62244965\n",
      "Iteration 84, loss = 1.62028376\n",
      "Iteration 85, loss = 1.61807983\n",
      "Iteration 86, loss = 1.61588233\n",
      "Iteration 87, loss = 1.61369158\n",
      "Iteration 88, loss = 1.61150787\n",
      "Iteration 89, loss = 1.60933142\n",
      "Iteration 90, loss = 1.60716244\n",
      "Iteration 91, loss = 1.60500110\n",
      "Iteration 92, loss = 1.60284753\n",
      "Iteration 93, loss = 1.60070185\n",
      "Iteration 94, loss = 1.59856415\n",
      "Iteration 95, loss = 1.59643451\n",
      "Iteration 96, loss = 1.59431299\n",
      "Iteration 97, loss = 1.59219962\n",
      "Iteration 98, loss = 1.59010832\n",
      "Iteration 99, loss = 1.58805646\n",
      "Iteration 100, loss = 1.58601565\n",
      "Iteration 101, loss = 1.58398551\n",
      "Iteration 102, loss = 1.58199511\n",
      "Iteration 103, loss = 1.58002761\n",
      "Iteration 104, loss = 1.57807220\n",
      "Iteration 105, loss = 1.57612832\n",
      "Iteration 106, loss = 1.57419545\n",
      "Iteration 107, loss = 1.57227315\n",
      "Iteration 108, loss = 1.57036098\n",
      "Iteration 109, loss = 1.56845857\n",
      "Iteration 110, loss = 1.56656556\n",
      "Iteration 111, loss = 1.56468164\n",
      "Iteration 112, loss = 1.56280650\n",
      "Iteration 113, loss = 1.56093989\n",
      "Iteration 114, loss = 1.55908155\n",
      "Iteration 115, loss = 1.55723125\n",
      "Iteration 116, loss = 1.55538879\n",
      "Iteration 117, loss = 1.55355397\n",
      "Iteration 118, loss = 1.55177286\n",
      "Iteration 119, loss = 1.55002918\n",
      "Iteration 120, loss = 1.54829682\n",
      "Iteration 121, loss = 1.54657516\n",
      "Iteration 122, loss = 1.54486360\n",
      "Iteration 123, loss = 1.54316162\n",
      "Iteration 124, loss = 1.54146875\n",
      "Iteration 125, loss = 1.53978455\n",
      "Iteration 126, loss = 1.53810863\n",
      "Iteration 127, loss = 1.53641252\n",
      "Iteration 128, loss = 1.53471490\n",
      "Iteration 129, loss = 1.53302204\n",
      "Iteration 130, loss = 1.53133399\n",
      "Iteration 131, loss = 1.52965078\n",
      "Iteration 132, loss = 1.52797241\n",
      "Iteration 133, loss = 1.52629892\n",
      "Iteration 134, loss = 1.52463031\n",
      "Iteration 135, loss = 1.52296659\n",
      "Iteration 136, loss = 1.52130775\n",
      "Iteration 137, loss = 1.51965379\n",
      "Iteration 138, loss = 1.51800471\n",
      "Iteration 139, loss = 1.51636048\n",
      "Iteration 140, loss = 1.51472111\n",
      "Iteration 141, loss = 1.51308854\n",
      "Iteration 142, loss = 1.51146255\n",
      "Iteration 143, loss = 1.50984634\n",
      "Iteration 144, loss = 1.50825903\n",
      "Iteration 145, loss = 1.50667793\n",
      "Iteration 146, loss = 1.50511546\n",
      "Iteration 147, loss = 1.50358358\n",
      "Iteration 148, loss = 1.50205961\n",
      "Iteration 149, loss = 1.50054465\n",
      "Iteration 150, loss = 1.49903856\n",
      "Iteration 151, loss = 1.49753923\n",
      "Iteration 152, loss = 1.49604636\n",
      "Iteration 153, loss = 1.49455967\n",
      "Iteration 154, loss = 1.49307893\n",
      "Iteration 155, loss = 1.49160388\n",
      "Iteration 156, loss = 1.49013434\n",
      "Iteration 157, loss = 1.48867043\n",
      "Iteration 158, loss = 1.48721329\n",
      "Iteration 159, loss = 1.48576088\n",
      "Iteration 160, loss = 1.48427498\n",
      "Iteration 161, loss = 1.48279059\n",
      "Iteration 162, loss = 1.48130881\n",
      "Iteration 163, loss = 1.47982590\n",
      "Iteration 164, loss = 1.47834418\n",
      "Iteration 165, loss = 1.47686403\n",
      "Iteration 166, loss = 1.47538659\n",
      "Iteration 167, loss = 1.47391166\n",
      "Iteration 168, loss = 1.47243890\n",
      "Iteration 169, loss = 1.47096850\n",
      "Iteration 170, loss = 1.46950062\n",
      "Iteration 171, loss = 1.46803682\n",
      "Iteration 172, loss = 1.46657547\n",
      "Iteration 173, loss = 1.46511674\n",
      "Iteration 174, loss = 1.46369349\n",
      "Iteration 175, loss = 1.46227646\n",
      "Iteration 176, loss = 1.46086504\n",
      "Iteration 177, loss = 1.45945849\n",
      "Iteration 178, loss = 1.45805665\n",
      "Iteration 179, loss = 1.45665940\n",
      "Iteration 180, loss = 1.45526889\n",
      "Iteration 181, loss = 1.45389285\n",
      "Iteration 182, loss = 1.45251273\n",
      "Iteration 183, loss = 1.45113611\n",
      "Iteration 184, loss = 1.44976877\n",
      "Iteration 185, loss = 1.44843205\n",
      "Iteration 186, loss = 1.44710170\n",
      "Iteration 187, loss = 1.44577659\n",
      "Iteration 188, loss = 1.44445650\n",
      "Iteration 189, loss = 1.44316601\n",
      "Iteration 190, loss = 1.44188327\n",
      "Iteration 191, loss = 1.44060659\n",
      "Iteration 192, loss = 1.43933621\n",
      "Iteration 193, loss = 1.43807088\n",
      "Iteration 194, loss = 1.43681062\n",
      "Iteration 195, loss = 1.43555567\n",
      "Iteration 196, loss = 1.43430539\n",
      "Iteration 197, loss = 1.43305960\n",
      "Iteration 198, loss = 1.43181811\n",
      "Iteration 199, loss = 1.43058077\n",
      "Iteration 200, loss = 1.42934744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import the classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "##Creating an instance of a MLP classifier\n",
    "#and setting it some option (max mum epoch, verbose on, activation of neurons)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(2,2), max_iter=200, verbose=True, activation='relu')\n",
    "\n",
    "#train the model\n",
    "mlp.fit(feat_train, lab_train)\n",
    "\n",
    "#applying the the model on the test data (features)\n",
    "lab_predict = mlp.predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'model_pickle.sav'\n",
    "pickle.dump(mlp,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mData\u001b[m\u001b[m/                MIDI.ipynb           Torch Intro.ipynb\r\n",
      "Day1.ipynb           Main Notebook.ipynb  Untitled.ipynb\r\n",
      "Day2.ipynb           Matplotlib.ipynb     array.npy\r\n",
      "Day3.ipynb           NumPy.ipynb          dataframe.xlsx\r\n",
      "Day3_2.ipynb         Pandas.ipynb         dataset.xlsx\r\n",
      "Day3_oldest.ipynb    Perceptron.ipynb     finalized_model.sav\r\n",
      "Day4.ipynb           Preprocessing.ipynb  foo.npy\r\n",
      "Day5b.ipynb          ReadMe.md            model.sav\r\n",
      "Intro.ipynb          SVM.ipynb            model_pickle.sav\r\n",
      "KNN.ipynb            SciPy.ipynb\r\n",
      "LibrosaTut.ipynb     Scikit-learn.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled samples 24 out of 31\n",
      "Accuracy: 0.22580645161290322\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00        10\n",
      "        1.0       0.00      0.00      0.00        14\n",
      "        2.0       0.23      1.00      0.37         7\n",
      "\n",
      "avg / total       0.05      0.23      0.08        31\n",
      "\n",
      "confusion matrix\n",
      "[[ 0  0 10]\n",
      " [ 0  0 14]\n",
      " [ 0  0  7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#plotting the loss curve over training iteration \n",
    "plt.plot(mlp.loss_curve_)\n",
    "plt.xlabel('iteration')\n",
    "plt.xlabel('loss')\n",
    "plt.show()\n",
    "\n",
    "#print the number of misclassified samples, accuracy and complete report (using scikit learn metric tools) \n",
    "print('Number of mislabeled samples %d out of %d' % ((lab_test != lab_predict).sum(),lab_test.size))\n",
    "print('Accuracy:',sklearn.metrics.accuracy_score(lab_test, lab_predict))\n",
    "print(sklearn.metrics.classification_report(lab_test, lab_predict))\n",
    "print('confusion matrix')\n",
    "print(sklearn.metrics.confusion_matrix(lab_test,lab_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the code below is taken from [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) and it simply facilitates the visualization of the confusion matrix (ignore it and focus on the visual outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 0  0 10]\n",
      " [ 0  0 14]\n",
      " [ 0  0  7]]\n",
      "Normalized confusion matrix\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fedining a function for the confusion matrix display \n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[sklearn.utils.multiclass.unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(lab_test, lab_predict, classes=classes,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(lab_test, lab_predict, classes=classes, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "a) Analyze in details the results displayed above and reconcile these with the classification task and dataset. Look at the accuracy, what gets missclassifies, inspect the trainig process (verbose mode is ON), and the displayed messages.\n",
    "\n",
    "b) sketch (on paper) the network we are using here (and repeat this everytime you modify the network structure).\n",
    "\n",
    "c) start to modify the network to improve the accuracy. You can explore increasing/decreasing the number of layers, the number of neuron per layer, the maximum number of training iterations, the activation function of the neurons (try logistic or tanh), and any other option you think may be relevant (refer to the documentation of MLPClassifier).\n",
    "\n",
    "d) report the network that give you the best performance (the smaller the network the better). One strategy is to gradually grow the network rather than immediately jumping to a large one (you may think to write a loop for this for your project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for regression\n",
    "The [MLPRegressor in scikit learn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation. \n",
    "\n",
    "MLPRegressor also supports multi-output regression, in which a sample can have more than one target.\n",
    "\n",
    "In the example below we use the same data we used for classification (it's not a great example but the aim here is to note the difference). You will change this in the following exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.72146959\n",
      "Iteration 2, loss = 2.69615949\n",
      "Iteration 3, loss = 2.67105314\n",
      "Iteration 4, loss = 2.64615282\n",
      "Iteration 5, loss = 2.62146066\n",
      "Iteration 6, loss = 2.59697871\n",
      "Iteration 7, loss = 2.57270886\n",
      "Iteration 8, loss = 2.54865287\n",
      "Iteration 9, loss = 2.52481236\n",
      "Iteration 10, loss = 2.50119874\n",
      "Iteration 11, loss = 2.47783239\n",
      "Iteration 12, loss = 2.45468638\n",
      "Iteration 13, loss = 2.43176133\n",
      "Iteration 14, loss = 2.40905559\n",
      "Iteration 15, loss = 2.38657108\n",
      "Iteration 16, loss = 2.36430873\n",
      "Iteration 17, loss = 2.34226881\n",
      "Iteration 18, loss = 2.32046570\n",
      "Iteration 19, loss = 2.29888761\n",
      "Iteration 20, loss = 2.27753174\n",
      "Iteration 21, loss = 2.25639596\n",
      "Iteration 22, loss = 2.23548184\n",
      "Iteration 23, loss = 2.21478883\n",
      "Iteration 24, loss = 2.19431625\n",
      "Iteration 25, loss = 2.17406332\n",
      "Iteration 26, loss = 2.15402919\n",
      "Iteration 27, loss = 2.13421286\n",
      "Iteration 28, loss = 2.11461328\n",
      "Iteration 29, loss = 2.09522928\n",
      "Iteration 30, loss = 2.07606687\n",
      "Iteration 31, loss = 2.05712131\n",
      "Iteration 32, loss = 2.03838749\n",
      "Iteration 33, loss = 2.01986384\n",
      "Iteration 34, loss = 2.00154871\n",
      "Iteration 35, loss = 1.98344041\n",
      "Iteration 36, loss = 1.96553720\n",
      "Iteration 37, loss = 1.94783731\n",
      "Iteration 38, loss = 1.93033891\n",
      "Iteration 39, loss = 1.91304016\n",
      "Iteration 40, loss = 1.89593916\n",
      "Iteration 41, loss = 1.87903400\n",
      "Iteration 42, loss = 1.86232274\n",
      "Iteration 43, loss = 1.84580342\n",
      "Iteration 44, loss = 1.82947405\n",
      "Iteration 45, loss = 1.81333263\n",
      "Iteration 46, loss = 1.79737717\n",
      "Iteration 47, loss = 1.78160563\n",
      "Iteration 48, loss = 1.76601600\n",
      "Iteration 49, loss = 1.75060624\n",
      "Iteration 50, loss = 1.73537432\n",
      "Iteration 51, loss = 1.72031821\n",
      "Iteration 52, loss = 1.70543588\n",
      "Iteration 53, loss = 1.69072531\n",
      "Iteration 54, loss = 1.67620056\n",
      "Iteration 55, loss = 1.66185390\n",
      "Iteration 56, loss = 1.64767410\n",
      "Iteration 57, loss = 1.63365892\n",
      "Iteration 58, loss = 1.61980619\n",
      "Iteration 59, loss = 1.60611374\n",
      "Iteration 60, loss = 1.59257947\n",
      "Iteration 61, loss = 1.57920128\n",
      "Iteration 62, loss = 1.56597713\n",
      "Iteration 63, loss = 1.55290501\n",
      "Iteration 64, loss = 1.53998292\n",
      "Iteration 65, loss = 1.52720891\n",
      "Iteration 66, loss = 1.51458106\n",
      "Iteration 67, loss = 1.50209747\n",
      "Iteration 68, loss = 1.48975627\n",
      "Iteration 69, loss = 1.47755678\n",
      "Iteration 70, loss = 1.46550091\n",
      "Iteration 71, loss = 1.45358226\n",
      "Iteration 72, loss = 1.44180084\n",
      "Iteration 73, loss = 1.43015587\n",
      "Iteration 74, loss = 1.41864293\n",
      "Iteration 75, loss = 1.40726024\n",
      "Iteration 76, loss = 1.39600609\n",
      "Iteration 77, loss = 1.38487876\n",
      "Iteration 78, loss = 1.37387660\n",
      "Iteration 79, loss = 1.36299795\n",
      "Iteration 80, loss = 1.35224123\n",
      "Iteration 81, loss = 1.34160484\n",
      "Iteration 82, loss = 1.33108723\n",
      "Iteration 83, loss = 1.32068687\n",
      "Iteration 84, loss = 1.31040225\n",
      "Iteration 85, loss = 1.30023191\n",
      "Iteration 86, loss = 1.29017439\n",
      "Iteration 87, loss = 1.28022825\n",
      "Iteration 88, loss = 1.27039208\n",
      "Iteration 89, loss = 1.26066451\n",
      "Iteration 90, loss = 1.25104416\n",
      "Iteration 91, loss = 1.24152968\n",
      "Iteration 92, loss = 1.23211977\n",
      "Iteration 93, loss = 1.22281310\n",
      "Iteration 94, loss = 1.21360841\n",
      "Iteration 95, loss = 1.20450442\n",
      "Iteration 96, loss = 1.19549988\n",
      "Iteration 97, loss = 1.18659357\n",
      "Iteration 98, loss = 1.17778428\n",
      "Iteration 99, loss = 1.16907082\n",
      "Iteration 100, loss = 1.16045201\n",
      "Iteration 101, loss = 1.15192669\n",
      "Iteration 102, loss = 1.14349372\n",
      "Iteration 103, loss = 1.13515198\n",
      "Iteration 104, loss = 1.12690033\n",
      "Iteration 105, loss = 1.11873420\n",
      "Iteration 106, loss = 1.11065575\n",
      "Iteration 107, loss = 1.10266556\n",
      "Iteration 108, loss = 1.09476149\n",
      "Iteration 109, loss = 1.08694214\n",
      "Iteration 110, loss = 1.07921674\n",
      "Iteration 111, loss = 1.07157782\n",
      "Iteration 112, loss = 1.06402128\n",
      "Iteration 113, loss = 1.05654603\n",
      "Iteration 114, loss = 1.04915099\n",
      "Iteration 115, loss = 1.04183005\n",
      "Iteration 116, loss = 1.03458580\n",
      "Iteration 117, loss = 1.02741940\n",
      "Iteration 118, loss = 1.02032952\n",
      "Iteration 119, loss = 1.01331445\n",
      "Iteration 120, loss = 1.00637329\n",
      "Iteration 121, loss = 0.99950514\n",
      "Iteration 122, loss = 0.99270911\n",
      "Iteration 123, loss = 0.98598433\n",
      "Iteration 124, loss = 0.97932996\n",
      "Iteration 125, loss = 0.97274514\n",
      "Iteration 126, loss = 0.96622907\n",
      "Iteration 127, loss = 0.95978091\n",
      "Iteration 128, loss = 0.95339989\n",
      "Iteration 129, loss = 0.94708521\n",
      "Iteration 130, loss = 0.94083610\n",
      "Iteration 131, loss = 0.93465179\n",
      "Iteration 132, loss = 0.92853154\n",
      "Iteration 133, loss = 0.92247461\n",
      "Iteration 134, loss = 0.91648027\n",
      "Iteration 135, loss = 0.91054780\n",
      "Iteration 136, loss = 0.90467649\n",
      "Iteration 137, loss = 0.89886565\n",
      "Iteration 138, loss = 0.89311632\n",
      "Iteration 139, loss = 0.88743319\n",
      "Iteration 140, loss = 0.88180893\n",
      "Iteration 141, loss = 0.87624280\n",
      "Iteration 142, loss = 0.87073409\n",
      "Iteration 143, loss = 0.86528209\n",
      "Iteration 144, loss = 0.85988611\n",
      "Iteration 145, loss = 0.85454548\n",
      "Iteration 146, loss = 0.84925954\n",
      "Iteration 147, loss = 0.84402764\n",
      "Iteration 148, loss = 0.83884916\n",
      "Iteration 149, loss = 0.83372346\n",
      "Iteration 150, loss = 0.82864993\n",
      "Iteration 151, loss = 0.82362798\n",
      "Iteration 152, loss = 0.81865702\n",
      "Iteration 153, loss = 0.81373646\n",
      "Iteration 154, loss = 0.80886573\n",
      "Iteration 155, loss = 0.80404427\n",
      "Iteration 156, loss = 0.79927153\n",
      "Iteration 157, loss = 0.79454696\n",
      "Iteration 158, loss = 0.78987003\n",
      "Iteration 159, loss = 0.78524157\n",
      "Iteration 160, loss = 0.78066815\n",
      "Iteration 161, loss = 0.77614130\n",
      "Iteration 162, loss = 0.77166044\n",
      "Iteration 163, loss = 0.76722501\n",
      "Iteration 164, loss = 0.76283442\n",
      "Iteration 165, loss = 0.75848815\n",
      "Iteration 166, loss = 0.75418566\n",
      "Iteration 167, loss = 0.74992642\n",
      "Iteration 168, loss = 0.74570992\n",
      "Iteration 169, loss = 0.74153568\n",
      "Iteration 170, loss = 0.73740320\n",
      "Iteration 171, loss = 0.73331200\n",
      "Iteration 172, loss = 0.72926162\n",
      "Iteration 173, loss = 0.72525159\n",
      "Iteration 174, loss = 0.72128146\n",
      "Iteration 175, loss = 0.71735079\n",
      "Iteration 176, loss = 0.71345915\n",
      "Iteration 177, loss = 0.70960610\n",
      "Iteration 178, loss = 0.70579122\n",
      "Iteration 179, loss = 0.70201409\n",
      "Iteration 180, loss = 0.69827432\n",
      "Iteration 181, loss = 0.69457149\n",
      "Iteration 182, loss = 0.69090520\n",
      "Iteration 183, loss = 0.68727508\n",
      "Iteration 184, loss = 0.68368072\n",
      "Iteration 185, loss = 0.68011664\n",
      "Iteration 186, loss = 0.67658553\n",
      "Iteration 187, loss = 0.67308856\n",
      "Iteration 188, loss = 0.66962541\n",
      "Iteration 189, loss = 0.66619580\n",
      "Iteration 190, loss = 0.66279941\n",
      "Iteration 191, loss = 0.65943594\n",
      "Iteration 192, loss = 0.65610509\n",
      "Iteration 193, loss = 0.65280656\n",
      "Iteration 194, loss = 0.64954004\n",
      "Iteration 195, loss = 0.64630522\n",
      "Iteration 196, loss = 0.64310182\n",
      "Iteration 197, loss = 0.63992951\n",
      "Iteration 198, loss = 0.63678802\n",
      "Iteration 199, loss = 0.63367703\n",
      "Iteration 200, loss = 0.63059626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import the classifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "##Creating an instance of a MLP classifier\n",
    "#and setting it some option (max mum epoch, verbose on, activation of neurons)\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(2,2), max_iter=200, verbose=True, activation='relu')\n",
    "\n",
    "#train the model\n",
    "mlp.fit(feat_train, lab_train)\n",
    "\n",
    "#applying the the model on the test data (features)\n",
    "lab_predict = mlp.predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2365246258814175\n",
      "-0.8814979555034422\n"
     ]
    }
   ],
   "source": [
    "#plotting the loss curve over training iteration \n",
    "plt.plot(mlp.loss_curve_)\n",
    "plt.xlabel('iteration')\n",
    "plt.xlabel('loss')\n",
    "plt.show()\n",
    "\n",
    "#Displaying the mean squared error(MSE), and the R2 regression score (just another regression metric).\n",
    "#For the mse, the lower the value the better. MSE values are difficult to compare across different data set\n",
    "#R2 score is better for comparing regression performance on different dataset.\n",
    "#R2 best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).\n",
    "#A constant model that always predicts the expected value of the output, disregarding the input features,\n",
    "#would get a R^2 score of 0.0.\n",
    "print(sklearn.metrics.mean_squared_error(lab_test, lab_predict))\n",
    "print(sklearn.metrics.r2_score(lab_test, lab_predict, multioutput='variance_weighted') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "a) Display (print) the prediction produced as the output of the regression, and compare it with the output of the classifier. Explain the difference (if any). Focus on the numerical format rather than on the individual values.\n",
    "\n",
    "b) Once again, modify the network to improve the performance.\n",
    "\n",
    "c) Modify the daya to perform a more interesting prediction problem using MLPRegressor. Do not consider the labels of the samples, as we are not interested in classification anymore. Earlier we computed 5 features for each sample (the mean over the entire sample), so we have 5 values for each sample. Select three of these as the input of the network and two of these as the target output. Train the network and check the result.\n",
    "\n",
    "For instance we can use ['SpecCen','SpecCon','SpecFlat'] as input and ['ZC','RMS'] as output. The network will try to learn to predict the zero crossing rate (ZC) and the RMS power given the three spectral features.\n",
    "\n",
    "For a regression problem, you can have as many output as you want in the MLP (while in the MLPClassifier, everything is merged down to one output, which is sufficient to label the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio\t MLPClassifier\t Path\t classes\t cymbal_features\t cymbal_signals\t cymballabels\t dataset\t extract_features\t \n",
      "feat_test\t feat_train\t features\t filename\t ipd\t kick_features\t kick_signals\t kicklabels\t lab_predict\t \n",
      "lab_test\t lab_train\t labels\t librosa\t mlp\t ms\t np\t os\t pd\t \n",
      "pickle\t plot_confusion_matrix\t plt\t scipy\t sklearn\t snare_features\t snare_signals\t snarelabels\t train_test_split\t \n",
      "\n"
     ]
    }
   ],
   "source": [
    "who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
